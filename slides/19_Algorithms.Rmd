---
title: "Algorithms"
subtitle: "PH 290"
author: "Xiongtao Dai"
output:
  xaringan::moon_reader:
    css: [myslides.css, myslides-fonts.css]
    lib_dir: libs
    chakra: libs/remark-latest.min.js
    nature:
      beforeInit: "macros.js"
      highlightLines: false
      countIncrementalSlides: false
---

```{r xaringan-tile-view, echo=FALSE}
xaringanExtra::use_tile_view()
knitr::opts_chunk$set(
  cache=TRUE, fig.width = 6, fig.height = 6
)
```

## Outline

- TODO

---

## References

- [Introduction to Algorithms 3rd Edition](https://cran.r-project.org/doc/manuals/R-exts.html) by Cormen, Leiserson, Rivest, and Stein (CLRS)

---

## Algorithms

- Informally, an *algorithm* is a computational procedure (series of computational steps) that takes a set of values as input and produces a set of values as output

- An *"efficient"* algorithm is a smart algorithm that is either "fast" or "save memory"

- Space-time tradeoff: An algorithm trades increased space usage with decreased time

    - E.g., if you cache results, then space $\uparrow$ but time $\downarrow$

- Algorithms operates upon *data structures*, which are ways to organize data in the computer memory/harddisk. E.g.,

    - Array
    - Linked list
    - Hash table

---

## Insertion sort

- Input: An array `A` of numbers
- Output: Sorted version of `A`
- Algorithm: The insertion sort. Work from the left to the right, and insert the number into the right location on the left

Pseudocode:
```{r, eval=FALSE}
INSERTION-SORT(A)
for j = 2:length(A)
    key = A[j] # current card
    # Insert A[j] into the sorted sequence A[1:(j-1)]
    i = j - 1
    while (i > 0 and A[i] > key)
        A[i + 1] = A[i]
        i = i - 1
    A[i + 1] = key
```

---

## Correctness

Proof that this algorithm is correct through mathematical induction:
- (*Initialization*) In the first iteration, you pick `A[2]`, and the cards on the left (just `A[1]`) are sorted
- (*Maintenance*) 
    - (*Loop invariance*) As the algorithm progresses, for each card `A[j]` to be inserted, the cards on the left to it are sorted versions of the cards originally on the left to `A[j]`
    - After you insert card `A[j]`, the cards on the left to the next card `A[j+1]` are sorted versions of the cards originally on the left to `A[j+1]`
- (*Termination*) The algorithm terminates if no cards remains to be sorted (on the right of the sorted cards). So all cards are sorted

---

## Measuring efficiency

- We would need a theoretical model to measure the efficiency of algorithms, should they be invoked on inputs with a large size 

- The amount of time/space taken by an algorithm is determined by the *input size*, e.g., the array size $n$

- Assume we have a one-processor, *random-access machine (RAM) model

- The following primitive operations take constant amounts of time
    - read/write a single number into a given address in the memory (e.g., write an array entry)
    - add, subtract, multiply, divide, remainder, floor, ceiling
    - raising powers, taking exponential, taking logarithm
    - control flows (if/else branches, function calls, return)

- The run time for these operations are comparable, and we assume all are the same

- The *cost in time* of the algorithm is measured by the number of primitive operations executed

---

## Worst- and average-case analysis

- *Average-case* time complexity is the cost in time averaged over the distribution of user inputs. This is what we care about in practice

- *Worst-case* time complexity gives an upper bound on the running time for any input. It gives the time cost under an adversarial input

- *Best-case* analysis is not very useful

---

## Order of growth

Asymptotic notations:

- An algorithm takes $\Theta(g(n))$ time if its run time $T(n)$ scales like $g(n)$ (i.e., $c_1 g(n) \le T(n) \le c_2 g(n)$ for some $0<c_1<c_2<\infty$ for large enough $n$). E.g., the average time complexity of insertion sort is $\Theta(n^2)$

- An algorithm takes $O(g(n))$ time if its run time is at most a constant multiply of $g(n)$ (between $c_1 g(n)$ and $c_2 g(n)$ for some $0<c_1<c_2<\infty$). The average time complexity of insertion sort is $O(n^2)$

- Worst-case and average-case analyses may have different time complexities

---

## The divide-and-conquer approach

- Many problems can be solved via solving smaller subproblems, the latter of which are much easier to solve

- This type of problems can benefit from a divide-and-conquer approach

- The subproblems are often similar to the main problem in nature, so if you have an algorithm to solve the subproblem, you can use that to build a solution for the main problem. This is called a recursive algorithm

---

- E.g., the following is a recursive algorithm to find the nth Fibonacci number. The first few values of the Fibonacci numbers are: $F_0=0$, $F_1=1$, $F_2=1$, $F_3=2$, $F_4=3$, $F_5=5$, ... (the nth number is the sum of the two numbers preceeding it)
    ```{r}
    fib <- function(n) {
        if (n >= 2) {
            fib(n - 1) + fib(n - 2)
        } else if (n == 1) {
            1
        } else if (n == 0) {
            0
        }
    }
    ```

---

## E.g., binary search

- Input: A value `v` and an array `A` of numbers. `A` is sorted in an increasing order
- Output: The first index `i` such that `v=A[i]`, or `-Inf` if `v` does not exist in `A`
- Algorithm: The binary search. Idea: Bipartite the array, and search in only one half of the array

```{r, eval=FALSE}
BINARY-SEARCH(A, v)
    cand = floor(length(A) / 2)
    if (A[cand] >= v) {
        i = BINARY-SEARCH(A[1:cand], v)
    } else {
        i = BINARY-SEARCH(A[(cand+1):length(A)], v)
    } ## WRONG! does not terminate if v is not found
```

---

```{r, eval=FALSE}
BINARY-SEARCH(A, v)
    cand = floor(length(A) / 2)
    if (length(A) == 1) {
        if (A == v) {
            return(indexOf(v))
        } else {
            return(-Inf)
        }
    } else if (A[cand] > v) {
        i = BINARY-SEARCH(A[1:cand], v)
    } else {
        i = BINARY-SEARCH(A[(cand+1):length(A)], v)
    }
```

---

## Correctness

- The subarray we search is the first subarray that may contain value `v`

- Every time we create a partition, we search in the first subarray that may contain `v`

- The algorithm terminates if there is only one element in a subarray. It either finds the first index i so that `A[i]=v`,  or if `v` does not exist in `A`, a negative number

---

## Time complexity analysis

- Let $T(n)$ be the worst-case time cost required for BINARY-SEARCH to search an array with length $n$

- Then $T(n) = T(n / 2) + c_0$, and $T(1) = c_1$ where $c_0, c_1$ are positive constants (like 5)
    - $T(1) = c_1$, 
    - $T(2) = c_1 + c_0$, 
    - $T(4) = c_1 + 2c_0$, 
    - $T(8) = c_1+ 3c_0$, 
    - $T(16) = c_1 + 4 c_0$, ...

- Turns out $T(n) = \Theta(\log(n))$ for binary search

---

## Analytic derivation

From the book of CLRS:
.center[
![:scale 90%](images/masterThm.png)
]

---

## Code implementation

R implementation (does not have time scaled as intended. Why?)

```{r}
binarySearch <- function(A, v) {
    cand = floor(length(A) / 2)
    if (length(A) == 1) {
        if (A == v) {
            return(1L)
        } else {
            return(-Inf)
        }
    } else if (A[cand] >= v) {
        binarySearch(A[1:cand], v)
    } else {
        binarySearch(A[(cand+1):length(A)], v) + cand
    }
}
```

---

C++ implementation `binarySearch.cpp`:
```
#include <cmath>
#include <Rcpp.h>
using namespace Rcpp;

int bs (double* A, int n, double v) {
    int cand = n / 2;
    if (n == 1) {
        if (A[0] == v) {
            return(0);
        } else {
            return(-n);
        }
    } else if (A[cand - 1] >= v) {
        return(bs(A, cand, v));
    } else {
        return(bs(A + cand, n - cand, v) + cand);
    }
}

// [[Rcpp::export()]]
int binarySearchCpp(NumericVector A, double v) {
  return(bs(A.begin(), A.size(), v) + 1);
}
```
---

In R:

```{r, eval=FALSE}
library(Rcpp)
sourceCpp("binarySearch.cpp")

res <- bench::press(n = 10^(4:7), {
  x <- sort(rnorm(n))
  v <- sample(x, 1)
  bench::mark(
    which(x == v),
    binarySearch(x, v), 
    binarySearchCpp(x, v)
  )
})
ggplot2::autoplot(res)
```

- `which` in R uses a linear search. It grows linearly
- R implementation `binarySearch()` copies data

---

class: inverse

## Your turn

- Implement an algorithm the returns all of the elements in an array $A$ that is smaller than $v$ (exclusive). We assume the array $A$ is sorted. $v$ does not have to be an element of $A$

- Find the time complexity of the algorithm

---

class: inverse

## Your turn (Partial solution)

Complexity: 
- Invoke a modified version of the binary search algorithm. Even if $v$ does not exists in $A$, return the index $i$ of the last search element. This step takes $\Theta(\log(n))$. 
    - If $i =1$, no element in $A$ is less than $v$
    - If $i > 1$, we must have $A[i - 1] < v$ and $A[i] >= v$

- Then returns $A[1], \dots, A[i-1]$. This step takes $i-1$ operations, where $i-1$ is the number of elements in $A$ that are smaller than $v$. Worst case is $n$ operation and the average case is $n/2$

---

## Quicksort

- *Quicksort* is a fast algorithm to sort an array 

- Its average-time complexity is $\Theta(n\log(n))$, and the worst-case complexity is $\Theta(n^2)$

- It performs sorting *in-place*, i.e., only a small and constant amount of memory is required to store temperary variables

- Idea: 
    - Divide: Partition the array around a pivot $A[q]$, so that the subarray left to the pivot are all no larger than $A[q]$, and the subarray right to $A[q]$ are no smaller than $A[q]$
    - Conquer: Sort the two subarrays by recursively calling quicksort

---

## Quicksort algorithm

```{r,eval=FALSE}
# Sort subarray A[p:r] in place
QUICKSORT(A, p, r)
if (p < r)
    q = PARTITION(A, p, r) # q is the index for the pivot
    QUICKSORT(A, p, q-1)
    QUICKSORT(A, q+1, r)
```

```{r,eval=FALSE}
PARTITION(A, p, r)
x = A[r]
i = p - 1
for j = p:(r - 1)
    if (A[j] <= x)
        i = i + 1
        swap(A[i], A[j])
swap(A[i+1], A[r])
return(i + 1)
```

---

## Time analysis

- Partitioning an array of length $n$ takes $\Theta(n)$ time

- Worst-case scenario happens when the array is already sorted. Everytime the partition will result in an empty right-subarray and a left-subarray containing all but the pivot. There will be $n$ partitions needed. So the complexity is $\Theta((n-1) + (n-2) + \dots + 1) = \Theta(n(n-1)/2) = \Theta(n^2)$.

- Best-case scenario happends when the pivots are all medians. Each partition results in two subarrays roughly half as large as the unpartitioned array. So $T(n) = 2 T(n-1) + \Theta(n - 1)$. By the master theorem $T(n) = \Theta(n\log(n))$

- The average-case time complexity is also $\Theta(n\log(n))$, since really bad partitions are rare to occur (see CLRS Chp 7)
