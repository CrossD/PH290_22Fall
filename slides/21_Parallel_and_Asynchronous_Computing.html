<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Parallel and Asynchronous Computing</title>
    <meta charset="utf-8" />
    <meta name="author" content="Xiongtao Dai" />
    <script src="libs/header-attrs-2.16/header-attrs.js"></script>
    <link href="libs/tile-view-0.2.6/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view-0.2.6/tile-view.js"></script>
    <link rel="stylesheet" href="myslides.css" type="text/css" />
    <link rel="stylesheet" href="myslides-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Parallel and Asynchronous Computing
]
.subtitle[
## PH 290
]
.author[
### Xiongtao Dai
]

---




## Outline

- Basics for parallel computing

- Futures and asynchronous programming

---

## References

- [Futureverse](https://www.futureverse.org/) and the packages therein

---

## Hardware

.center[
![:scale 50%](images/computingArchitecture2.png)
]

- A *node* is a physical machine. On a *high performance computing (HPC)* cluster, there are usually multiple nodes, and nodes are interconnected with fast internet
    - This lecture focuses on parallelism on a single machine; the next is about HPC

- Within each node, multiple *cores/CPUs* are available. They share the same memory
    &lt;!-- - Passing information between nodes require special code (message passing interface MPI) --&gt;

---

- Sometimes a core may have two (hyper)*threads*. The two threads can sometimes better utilize the idle time of a single core. However, this notion is not very useful in parallel computing
    - Using only 1 thread per core usually gives maximal performance
    - "Threads" have different meanings in different contexts. E.g., multi-threaded linear algebra means it uses multiple cores to do matrix algebra

- Hard disks are usually shared between nodes and are connected through high-speed internet

---

## Parallel jobs and dispatch

- Each job is typically an iteration of a loop. E.g., one out of 500 bootstrap iterations

- Basically, parallel computation consists of 
    - Send jobs to the worker processes
    - Workers perform the computation
    - Workers send back the results to the main process when done

- So the total time = time for communication + time for the slowest worker to finish computation 

- When and how the workers receive jobs depends on scheduling (later)

---

## Passing jobs around

- The main process needs to pass the jobs to the workers. A job consists of code (including functions and libraries) and data (inputs, and global variables) where the code depends on

There are a few strategies for launching workers sending jobs (in the terminology of `library(future)`):

On a single machine (pipelines are fast):

- `sequential`: No parallelism. Use a single core to execute iterations sequentially. No communication overhead

- `multisession`: Launch multiple R sessions, send data, run code, and collect results. Input data needs to be passed from the main process to the sessions, and results passed back. *Available on all systems*

- `multicore`: Uses `fork()` to launch multiple R sessions. The forked sessions share the memory of the main process and thus data do not need to be sent to the workers. This is the most efficient method for launching workers and sending data. *Not available on Windows/RStudio*

---

On multiple machines (more communication overhead):
- Cluster: Launch jobs on remote workers and send data through SSH

- SLURM: Have R request computing resources from a HPC and then dispatch jobs to the nodes/cores requested

---

## `library(future)`

- `library(future)` provides a unified framework for parallel and asynchronous computing
    - It let you separate *how* to parallelize from *what* to parallelize
        - How to parallel is specified by a parallel backend
        - What to parallel is specified by code that looks like `lapply` or `for` 

- `plan(...)` specifies a strategy for the (parallel) evaluation, providing a backend for subsequent parallel execution. Subsequent parallel computation will automatically use this backend. E.g., 
    - Launch 2 workers: `plan(multisession, worker=2)`
    - Sequentially evaluate the outer loop, and uses multicore to evaluate the inner loop:
    
    ```r
    plan(list(sequential,
              tweak(multicore, workers = availableCores())))
    ```

- `future.apply::future_lapply()` is like `lapply()` but will apply the specified evaluation strategy

---

E.g. of different scheduling 

```r
n &lt;- 1e2
m &lt;- 1e7
l &lt;- lapply(seq_len(n), function(i) rep(1, m))
system.time(lapply(l, sum))                      # 1.562s, single core

future::plan(sequential)
system.time(future.apply::future_lapply(l, sum)) # 1.565s, single core

future::plan(multisession, workers=2)
system.time(future.apply::future_lapply(l, sum)) # 7.738s on my mac

future::plan(multicore, workers=2)
system.time(future.apply::future_lapply(l, sum)) # 1.093s on my mac
```

- Even the most efficient `multicore` strategy won't cut the time in half when using two cores because of communication overhead, if the job is small

---

## Improving efficiency

Total time = time for communication + time for the slowest worker to finish computation 

- The larger granularity of the job (bigger jobs), the better
    - If you have two loops and have to parallelize only one of them, parallelize the outer loop
    - E.g., if your simulation involves multiple experiments and bootstrap, then have each worker runs one experiment with 500 bootstrap iterations
    - Group together small jobs

- The more balance the workloads are for the workers, the better
    - E.g., if your simulation involves multiple different sample sizes and multiple experiments for each sample size, parallelize over the experiments

---

## Scheduling tasks

- *Static scheduling* assigns jobs to the workers only once before the parallel code starts
    - This is ideal for *embarrassingly parallelized* jobs, namely jobs that are highly similar in nature. E.g., Monte Carlo iterations, one CV fold

- *Dynamic scheduling* assigns jobs during execution. Once a worker is done with a job, it is then sent a new one. This scheduling is also referred to as *load balancing*
    - Useful when the jobs have highly different run time

- *Chunking* means grouping jobs into larger ones as if they form a single job. 
    - In static scheduling, suppose there are `\(10\)` jobs and `\(2\)` workers in total. Then each worker gets a *chunk* of `\(5\)` jobs only once
    - In dynamic scheduling, each chunk consists of 1 job and there are 10 chunks in total
    - Strike a happy intermediate 

---

E.g., load balancing hurts when the jobs are similar in size and helps when the jobs are different in size:


```r
system.time(future_lapply(l, sum))# Static schedule    1.093s
system.time(future_lapply(l, sum,                 #   Dynamic
                          future.chunk.size = 1))    # 1.849s

system.time(future_lapply(c(.1, .1, 1, 1),          # Static
                          function(x) Sys.sleep(x))) # 2.030s
system.time(future_lapply(c(.1, .1, 1, 1), 
                          function(x) Sys.sleep(x), # Dynamic
                          future.chunk.size = 1))    # 1.162s
```

---

class: inverse

## Your turn

- Use `future::plan()` and `future.apply::future_lapply` to fit the model 200 times: 
```
glm(default~student+balance+income,family="binomial",
    data=ISLR2::Default)
``` 

- Play around with different plans


---

## Parallel random number generation

- Random seeds needs to be specially handled in parallel jobs for reproducibility and debugging

- The idea is to generate a sequence of random stream of seeds the same length as the jobs, and use one seed for each job

- This is done using `future_lapply(..., future.seed=TRUE)`
    
    ```r
    set.seed(1)
    future_lapply(c(1, 1), rnorm, future.seed=TRUE)
    ```
---

## Interaction with multithreaded BLAS

- Multi-threaded BLAS, e.g., Intel MKL, uses multiple cores in one R process

- This may with the parallel computation. Using more cores than a node results in reduced performance because the processes compete for resources

- If the workers are using multi-threaded BLAS, setting the BLAS to use only 1 thread if you have many jobs and can thus fully utilize all cores anyway (`RhpcBLASctl::blas_set_num_threads(1)`)

---

class: big, middle

## More on asynchronous programming

---

## Asynchronous programming

- Asynchronous programming is a form of parallel programming that allows code to be evaluated separatedly from the main process

- Useful for implementing the type of structure for "fast lanes" (at a supermarket checkout, say)
    - The slow lanes should not block the fast lanes

- `library(future)` offers a data structure called *future* to implement asynchronous programming. A future holds the value of an expression, which may be resolved or unresolved

- An unresolved future is like a parcel of code and data that can be evaluated elsewhere

- To resolve a future, we need to send the future to a worker to obtain the value

- A evaluation strategy (`plan()`) is needed for resolving a future, like in parallel programming

---

- `future()` creates a future that is sent to be evaluated by a parallel worker. This is *non-blocking*, which means you will regain control of the R console immediately
    
    ```r
    library(future)
    plan(multisession, workers=2)
    f1 &lt;- future({
        Sys.sleep(5)
        lm(extra ~ group, data=sleep)
    })
    ```

- Get the value of the future. If the future is resolved, you get the result immediately (no blocking); if not, the main process will be blocked until the future resolves
    
    ```r
    value(f1)
    ```

- If all workers are evaluating futures, then the creation of a new future will block the main process

- Non-blocking evaluation is useful for fitting new models while inspecting old ones; web crawling; waiting for long-running queries

---

## Global variables

- Global variables used in a future are usually automatically identified. This is done through `library(globals)` which performs static code analysis (without running the code)
    
    ```r
    x &lt;- 1
    value(future({
      y &lt;- x
      print(y)
    })) # 1
    ```

- Sometimes the automatic identification of global variables does not work, and you must specify it manually
    
    ```r
    x &lt;- 1
    value(future({
      y &lt;- get("x")
      print(y)
    })) # Error in get("x") : object 'x' not found
    value(future({
      y &lt;- get("x")
      print(y)
    }, globals=list(x=x))) # 1
    ```



    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightLines": false,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
